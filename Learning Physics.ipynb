{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olwflynn/Learning/blob/master/Learning%20Physics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s7PPg0VZG5cK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "039b6f5d-387b-4139-aefd-9ab1b702c6f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libgl1-mesa-dev is already the newest version (20.0.8-0ubuntu1~18.04.1).\n",
            "libgl1-mesa-dev set to manually installed.\n",
            "software-properties-common is already the newest version (0.96.24.32.18).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "Suggested packages:\n",
            "  glew-utils\n",
            "The following NEW packages will be installed:\n",
            "  libgl1-mesa-glx libglew-dev libglew2.0 libosmesa6 libosmesa6-dev\n",
            "0 upgraded, 5 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 2,916 kB of archives.\n",
            "After this operation, 12.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgl1-mesa-glx amd64 20.0.8-0ubuntu1~18.04.1 [5,532 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libglew2.0 amd64 2.0.0-5 [140 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libglew-dev amd64 2.0.0-5 [120 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libosmesa6 amd64 20.0.8-0ubuntu1~18.04.1 [2,641 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libosmesa6-dev amd64 20.0.8-0ubuntu1~18.04.1 [8,828 B]\n",
            "Fetched 2,916 kB in 0s (14.3 MB/s)\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack .../libgl1-mesa-glx_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Selecting previously unselected package libglew2.0:amd64.\n",
            "Preparing to unpack .../libglew2.0_2.0.0-5_amd64.deb ...\n",
            "Unpacking libglew2.0:amd64 (2.0.0-5) ...\n",
            "Selecting previously unselected package libglew-dev:amd64.\n",
            "Preparing to unpack .../libglew-dev_2.0.0-5_amd64.deb ...\n",
            "Unpacking libglew-dev:amd64 (2.0.0-5) ...\n",
            "Selecting previously unselected package libosmesa6:amd64.\n",
            "Preparing to unpack .../libosmesa6_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libosmesa6:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Selecting previously unselected package libosmesa6-dev:amd64.\n",
            "Preparing to unpack .../libosmesa6-dev_20.0.8-0ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libosmesa6-dev:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Setting up libosmesa6:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Setting up libglew2.0:amd64 (2.0.0-5) ...\n",
            "Setting up libglew-dev:amd64 (2.0.0-5) ...\n",
            "Setting up libosmesa6-dev:amd64 (20.0.8-0ubuntu1~18.04.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  patchelf\n",
            "0 upgraded, 1 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 46.5 kB of archives.\n",
            "After this operation, 130 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 patchelf amd64 0.9-1 [46.5 kB]\n",
            "Fetched 46.5 kB in 0s (953 kB/s)\n",
            "Selecting previously unselected package patchelf.\n",
            "(Reading database ... 124054 files and directories currently installed.)\n",
            "Preparing to unpack .../patchelf_0.9-1_amd64.deb ...\n",
            "Unpacking patchelf (0.9-1) ...\n",
            "Setting up patchelf (0.9-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common\n",
        "\n",
        "!apt-get install -y patchelf"
      ],
      "id": "s7PPg0VZG5cK"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5w7tslZCHl0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db420412-23fe-43a7-e485-2742e229e942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym) (5.2.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym) (1.21.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym) (3.11.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym"
      ],
      "id": "5w7tslZCHl0T"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3qeAEFtTG8dV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38230207-21ad-4cfb-b006-ed315679e5d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting free-mujoco-py\n",
            "  Downloading free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mujoco\n",
            "  Downloading mujoco-2.3.1.post1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Cython<0.30.0,>=0.29.24 in /usr/local/lib/python3.8/dist-packages (from free-mujoco-py) (0.29.32)\n",
            "Collecting fasteners==0.15\n",
            "  Downloading fasteners-0.15-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from free-mujoco-py) (1.15.1)\n",
            "Collecting glfw<2.0.0,>=1.4.0\n",
            "  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 KB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio<3.0.0,>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from free-mujoco-py) (2.9.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.8/dist-packages (from free-mujoco-py) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from fasteners==0.15->free-mujoco-py) (1.15.0)\n",
            "Collecting monotonic>=0.1\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.8/dist-packages (from mujoco) (3.1.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from mujoco) (1.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.21)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from imageio<3.0.0,>=2.9.0->free-mujoco-py) (7.1.2)\n",
            "Installing collected packages: monotonic, glfw, mujoco, fasteners, free-mujoco-py\n",
            "Successfully installed fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0 monotonic-1.6 mujoco-2.3.1.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install free-mujoco-py mujoco"
      ],
      "id": "3qeAEFtTG8dV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHtnurnyIwaZ"
      },
      "source": [
        "**Now you need to restart the runtime as numpy is apparently automatically imported...**"
      ],
      "id": "MHtnurnyIwaZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtWrmOCKHD6I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f079174-afa3-443d-8c63-2b818b38b774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling /usr/local/lib/python3.8/dist-packages/mujoco_py/cymj.pyx because it changed.\n",
            "[1/1] Cythonizing /usr/local/lib/python3.8/dist-packages/mujoco_py/cymj.pyx\n",
            "running build_ext\n",
            "building 'mujoco_py.cymj' extension\n",
            "creating /usr/local/lib/python3.8/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_38_linuxcpuextensionbuilder\n",
            "creating /usr/local/lib/python3.8/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_38_linuxcpuextensionbuilder/temp.linux-x86_64-3.8\n",
            "creating /usr/local/lib/python3.8/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_38_linuxcpuextensionbuilder/temp.linux-x86_64-3.8/usr\n",
            "creating /usr/local/lib/python3.8/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_38_linuxcpuextensionbuilder/temp.linux-x86_64-3.8/usr/local\n",
            "creating /usr/local/lib/python3.8/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_38_linuxcpuextensionbuilder/temp.linux-x86_64-3.8/usr/local/lib\n",
            "creating /usr/local/lib/python3.8/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_38_linuxcpuextensionbuilder/temp.linux-x86_64-3.8/usr/local/lib/python3.8\n",
            "creating /usr/local/lib/python3.8/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_38_linuxcpuextensionbuilder/temp.linux-x86_64-3.8/usr/local/lib/python3.8/dist-packages\n",
            "creating /usr/local/lib/python3.8/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_38_linuxcpuextensionbuilder/temp.linux-x86_64-3.8/usr/local/lib/python3.8/dist-packages/mujoco_py\n",
            "creating /usr/local/lib/python3.8/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_38_linuxcpuextensionbuilder/temp.linux-x86_64-3.8/usr/local/lib/python3.8/dist-packages/mujoco_py/gl\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/mujoco_py -I/usr/local/lib/python3.8/dist-packages/mujoco_py/binaries/linux/mujoco210/include -I/usr/local/lib/python3.8/dist-packages/numpy/core/include -I/usr/include/python3.8 -c /usr/local/lib/python3.8/dist-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.8/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_38_linuxcpuextensionbuilder/temp.linux-x86_64-3.8/usr/local/lib/python3.8/dist-packages/mujoco_py/cymj.o -fopenmp -w\n"
          ]
        }
      ],
      "source": [
        "import mujoco_py\n",
        "import gym"
      ],
      "id": "TtWrmOCKHD6I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad741ec6-c60b-4124-a3ef-80944f5d6f75",
      "metadata": {
        "id": "ad741ec6-c60b-4124-a3ef-80944f5d6f75"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# def plt_update(ax, colors=['b']):\n",
        "#     x = np.linspace(0,1,100)\n",
        "#     if ax.lines:\n",
        "#         for line in ax.lines:\n",
        "#             line.set_xdata(x)\n",
        "#             y = np.random.random(size=(100,1))\n",
        "#             line.set_ydata(y)\n",
        "#     else:\n",
        "#         for color in colors:\n",
        "#             y = np.random.random(size=(100,1))\n",
        "#             ax.plot(x, y, color)\n",
        "#     fig.canvas.draw()\n",
        "\n",
        "\n",
        "def live_plot(data_dict, figsize=(7,5), title=''):\n",
        "    # clear_output(wait=True)\n",
        "    plt.figure(figsize=figsize)\n",
        "    # for label,data in data_dict.items():\n",
        "    obs = data_dict[\"obs\"]\n",
        "    preds = data_dict[\"preds\"]\n",
        "    print(\"obs: \", obs, \"\\n preds: \",preds)\n",
        "\n",
        "    plt.plot(obs, label=\"obs\")\n",
        "    plt.plot(range(len(obs)-1, len(obs)+len(preds)-1), preds, label=\"preds\")\n",
        "    \n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(loc='center left') # the plot evolves to the right\n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to restart runtime if cant find 'mujuco'"
      ],
      "metadata": {
        "id": "tGiYgjBWd-Ha"
      },
      "id": "tGiYgjBWd-Ha"
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"InvertedDoublePendulum-v4\", new_step_api=True)\n",
        "env.reset(seed=42)\n",
        "env.step(env.action_space.sample())"
      ],
      "metadata": {
        "id": "7YlxRJWa1mYd"
      },
      "id": "7YlxRJWa1mYd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "665867d5-71f6-4b62-a604-a29c8f74f4bf",
      "metadata": {
        "id": "665867d5-71f6-4b62-a604-a29c8f74f4bf"
      },
      "outputs": [],
      "source": [
        "# naive forecaster where we predict the angle at the next 3 steps is the same as the current angle\n",
        "\n",
        "def naive_forecaster(X):\n",
        "    # X is the all data known to the forecaster at the time. X is only the current and previous angles for now.\n",
        "    \n",
        "    current_angle = X[-1]\n",
        "    angle_t_plus_1 = current_angle\n",
        "    angle_t_plus_2 = current_angle\n",
        "    angle_t_plus_3 = current_angle\n",
        "    \n",
        "    return np.array([angle_t_plus_1, angle_t_plus_2, angle_t_plus_3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8963723f-d1d8-46e8-a062-277326a5efaf",
      "metadata": {
        "id": "8963723f-d1d8-46e8-a062-277326a5efaf"
      },
      "outputs": [],
      "source": [
        "# get forecast error given a predicted trajectory and observations\n",
        "\n",
        "def forecast_error(X_obs, preds):\n",
        "    \n",
        "    mse = sum(np.square(X_obs - preds)) / len(X_obs)\n",
        "    \n",
        "    return mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf4b649a",
      "metadata": {
        "tags": [],
        "id": "cf4b649a"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# observation, info = env.reset(seed=42)\n",
        "obs_history = []\n",
        "# fig,ax = plt.subplots(1,1)\n",
        "# ax.set_xlabel('t')\n",
        "# ax.set_ylabel('obs_idx_1')\n",
        "# ax.set_xlim(0,20)\n",
        "# ax.set_ylim(-5,5)\n",
        "\n",
        "for _ in range(10):\n",
        "    print(\"epoch #\", _)\n",
        "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
        "    print(observation, reward, terminated, truncated, info)\n",
        "    obs_history.append(observation[1])\n",
        "    # plt.plot(obs_history[0][0])\n",
        "    # plt.show()\n",
        "    preds = obs_history[-1]+[0, 0.2,0.2,0.2]\n",
        "    # obs_concat_preds = obs_history\n",
        "    # for pred in preds:\n",
        "    #     obs_concat_preds.append(pred)\n",
        "    live_plot({\"obs\": obs_history, \"preds\": preds}, title=\"Radians by epoch\")\n",
        "    time.sleep(1)\n",
        "    if terminated or truncated:\n",
        "        print(obs_history)\n",
        "        print(\"Prediction error in last 3 steps: \", forecast_error(obs_history[-3:], preds[-3:]))\n",
        "        break\n",
        "        # obs_history = []\n",
        "        # observation, info = env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dac7f61-e9f7-40a1-9bee-b6fdd9b0668e",
      "metadata": {
        "id": "0dac7f61-e9f7-40a1-9bee-b6fdd9b0668e"
      },
      "source": [
        "The episode ends when any of the following happens:\n",
        "\n",
        "Truncation: The episode duration reaches 1000 timesteps.\n",
        "\n",
        "Termination: Any of the state space values is no longer finite.\n",
        "\n",
        "Termination: The absolutely value of the vertical angle between the pole and the cart is greater than 0.2 radian. i.e. 11.4592 degrees\n",
        "\n",
        "1; vertical angle of the pole on the cart; -Inf; Inf; angle (rad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30806ff9-8c40-4528-be89-8b210d99c109",
      "metadata": {
        "id": "30806ff9-8c40-4528-be89-8b210d99c109"
      },
      "source": [
        "*JTBD*\n",
        "\n",
        "- Run thousands of episodes to generate training data with features (current and prev angles and other data) and labels (next few angles)\n",
        "- Train forecast model to predict trajectory of angle\n",
        "- Start episode with control over speed i.e. time lag between steps\n",
        "- Predict expected time to end of episode at step t with measure of uncertainty\n",
        "- Predict trajectory of angle in next few time steps at step t\n",
        "- At end of episode output prediction errors at a few time steps\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "Ski4fdQA-Qu4"
      },
      "id": "Ski4fdQA-Qu4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eefda0be-faaa-436a-8b4b-62109c13f57d",
      "metadata": {
        "id": "eefda0be-faaa-436a-8b4b-62109c13f57d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "X = torch.randn(1, 15)\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.ones(1, 10)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "LkqGCLBT-O9P"
      },
      "id": "LkqGCLBT-O9P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y = softmax(layer2(layer1(X)))\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "class TestNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TestNetwork, self).__init__()\n",
        "    self.seqlayers = nn.Sequential(\n",
        "        nn.Linear(15, 20),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(20, 10),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self, X):\n",
        "    hidden1 = self.seqlayers(X)\n",
        "    output = self.softmax(hidden1)\n",
        "    return output\n",
        "\n"
      ],
      "metadata": {
        "id": "Q-C_ZzYz-xdb"
      },
      "id": "Q-C_ZzYz-xdb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TestNetwork()\n",
        "print(model)"
      ],
      "metadata": {
        "id": "smt_eLALBMZK"
      },
      "id": "smt_eLALBMZK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(X)\n",
        "preds"
      ],
      "metadata": {
        "id": "afx--8_RBT_s"
      },
      "id": "afx--8_RBT_s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds.size()"
      ],
      "metadata": {
        "id": "8iuSXNVTCNan"
      },
      "id": "8iuSXNVTCNan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_ = torch.nn.functional.binary_cross_entropy(preds, y)\n",
        "loss_"
      ],
      "metadata": {
        "id": "bJ7hcXQbCUck"
      },
      "id": "bJ7hcXQbCUck",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation for one step\n",
        "learning_rate = 0.1\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "loss_.backward()\n",
        "optimizer.step()\n",
        "\n",
        "new_preds = model(X)\n",
        "new_loss = torch.nn.functional.binary_cross_entropy(new_preds, y)\n",
        "\n",
        "print(new_loss)"
      ],
      "metadata": {
        "id": "hhgkljCwDRUG"
      },
      "id": "hhgkljCwDRUG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rows_for_episode(ep_num):\n",
        "  env.reset(seed=42)\n",
        "  for _ in range(100):\n",
        "    observation, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
        "    features = np.array(observation[:8]).reshape(1, 8)\n",
        "    if _ == 0:\n",
        "      rows_for_ep = features\n",
        "    else:\n",
        "      rows_for_ep = np.concatenate((rows_for_ep, features), axis=0)\n",
        "    if terminated or truncated:\n",
        "      labels = np.array(range(_, -1,-1)).reshape(_+1, 1)\n",
        "      ep_num_arr = np.array([ep_num for i in range(len(labels))]).reshape(_+1, 1)\n",
        "      assert len(labels) == len(rows_for_ep)\n",
        "      rows_for_ep = np.concatenate((rows_for_ep, labels), axis=1)\n",
        "      rows_for_ep = np.concatenate((rows_for_ep, ep_num_arr), axis=1)\n",
        "      # print(len(rows_for_ep), type(rows_for_ep))\n",
        "      return rows_for_ep\n",
        "  \n",
        "    "
      ],
      "metadata": {
        "id": "CCL0igDFRnWn"
      },
      "id": "CCL0igDFRnWn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataset with positions of the pendulum as features and time to terminate as label\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "columns = []\n",
        "for i in range(8):\n",
        "  columns.append(f'feature_{i}')\n",
        "columns.append('time_to_terminate')\n",
        "columns.append('ep_num')\n",
        "\n",
        "#create array\n",
        "num_episodes = 10000\n",
        "for j in range(num_episodes):\n",
        "  episode_rows = create_rows_for_episode(j)\n",
        "  if j == 0:\n",
        "    arr = episode_rows\n",
        "  else:\n",
        "    arr = np.concatenate([arr, episode_rows], axis=0)\n",
        "env.close()\n",
        "\n",
        "df = pd.DataFrame(arr, columns=columns)\n",
        "print(df.head())\n",
        "print(df.describe())\n",
        "print(df.info())"
      ],
      "metadata": {
        "id": "oiaRdUnNDpsn"
      },
      "id": "oiaRdUnNDpsn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# terminating conditions of the episodes\n",
        "\n",
        "df_terminated = df[df['time_to_terminate']==0]\n",
        "print(df_terminated.head())\n",
        "print(df_terminated.describe())\n",
        "print(df_terminated.info())"
      ],
      "metadata": {
        "id": "sd5H2CORBQ78"
      },
      "id": "sd5H2CORBQ78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot a five of the episodes against various features\n",
        "\n",
        "ep_split_df = [df[df['ep_num']==i] for i in range(5)]\n",
        "num_features = 8\n",
        "fig, axs = plt.subplots(num_features, figsize=(12, 8))\n",
        "plt.suptitle('First 5 episodes against features')\n",
        "for i in range(num_features):\n",
        "  for el in ep_split_df:\n",
        "    ep = el['ep_num'].values[0]\n",
        "    axs[i].plot(el[f'feature_{i}'], label=f'episode_{int(ep)}')\n",
        "    axs[i].set_title(f'feature_{i}', fontsize='small')\n",
        "# plt.legend(loc='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HG_LvX_NxKam"
      },
      "id": "HG_LvX_NxKam",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import triangular\n",
        "# prepare dataset for train, validate, test\n",
        "# use final 20% episodes for testing\n",
        "\n",
        "test_size = 0.2*num_episodes\n",
        "valid_size = 0.1*num_episodes\n",
        "train_size = num_episodes - (test_size+valid_size)\n",
        "\n",
        "test_df = df[df['ep_num']<=test_size]\n",
        "valid_df = df[(df['ep_num']>test_size)&(df['ep_num']<=(valid_size+test_size))]\n",
        "train_df = df[(df['ep_num']>(valid_size+test_size))]\n",
        "\n",
        "features = []\n",
        "for i in range(8):\n",
        "  features.append(f'feature_{i}')\n",
        "\n",
        "test_labels = test_df['time_to_terminate']\n",
        "test_features = test_df[features]\n",
        "\n",
        "valid_labels = valid_df['time_to_terminate']\n",
        "valid_features = valid_df[features]\n",
        "\n",
        "train_labels = train_df['time_to_terminate']\n",
        "train_features = train_df[features]\n",
        "\n",
        "print('Test dataset shapes:', test_labels.shape, test_features.shape)\n",
        "print('Validation dataset shapes:', valid_labels.shape, valid_features.shape)\n",
        "print('Train dataset shapes:', train_labels.shape, train_features.shape)"
      ],
      "metadata": {
        "id": "gmKXAZHJpzQ6"
      },
      "id": "gmKXAZHJpzQ6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# linear regression model to predict time_to_terminate using features as baseline\n",
        "# this model does not take into account the time series i.e. it assumes that t, t+1 are independent\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "regr = LinearRegression()\n",
        "\n",
        "regr.fit(train_features, train_labels)\n",
        "\n",
        "preds = regr.predict(test_features)\n",
        "\n",
        "print('MSE: \\n', forecast_error(test_labels, preds))\n",
        "print('Score: \\n', regr.score(test_features, test_labels))"
      ],
      "metadata": {
        "id": "DGVE0Lf1KHNi"
      },
      "id": "DGVE0Lf1KHNi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(regr.coef_)\n",
        "print(regr.intercept_)"
      ],
      "metadata": {
        "id": "E2sj7t90VOXp"
      },
      "id": "E2sj7t90VOXp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results:\n",
        "\n",
        "> n=100, mse=2.07, model=lin reg\n",
        "coef= [ 1.33349380e+00 -6.43072005e-01 -1.25702564e+00  6.48821446e+00\n",
        "  5.30884445e+00 -1.92036934e-01 -8.90950510e-02 -3.15731262e-03]\n",
        "intercept= -7.1225544550046775\n",
        "\n",
        "\n",
        "\n",
        "> n=10000, mse=2.07, model=lin reg\n",
        "coef=[ 8.05432692  6.7247005   2.55556171  4.98903376  5.38681353 -0.09618963\n",
        "  0.021658    0.03276889]\n",
        "intercept=-6.228216297398191\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hQ8aEqjZYMr9"
      },
      "id": "hQ8aEqjZYMr9"
    },
    {
      "cell_type": "code",
      "source": [
        "# log linear so only positve? \n",
        "# look at gamma regression to use linear regression and only predict positive values\n",
        "\n",
        "from sklearn.linear_model import PoissonRegressor\n",
        "\n",
        "pois_regr = PoissonRegressor()\n",
        "\n",
        "pois_regr.fit(train_features, train_labels)\n",
        "\n",
        "pois_preds = pois_regr.predict(test_features)\n",
        "print('Poisson model')\n",
        "print('MSE: \\n', forecast_error(test_labels, pois_preds))\n",
        "print('Score: \\n', pois_regr.score(test_features, test_labels))"
      ],
      "metadata": {
        "id": "GVuSeff4Wa6B"
      },
      "id": "GVuSeff4Wa6B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import TweedieRegressor\n",
        "\n",
        "tweed_regr = TweedieRegressor(power=1.5)\n",
        "\n",
        "tweed_regr.fit(train_features, train_labels)\n",
        "\n",
        "tweed_preds = tweed_regr.predict(test_features)\n",
        "print('Tweedie w/ power=1.5')\n",
        "print('MSE: \\n', forecast_error(test_labels, tweed_preds))\n",
        "print('Score: \\n', tweed_regr.score(test_features, test_labels))"
      ],
      "metadata": {
        "id": "2UXgupFJuCRu"
      },
      "id": "2UXgupFJuCRu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take linear model but set preds less than zero to 0.\n",
        "preds_positive = preds.copy()\n",
        "preds_positive[preds_positive < 0] =  0\n",
        "\n",
        "print('Positive only predictions linear model')\n",
        "print('MSE: \\n', forecast_error(test_labels, preds_positive))\n"
      ],
      "metadata": {
        "id": "vchbuD4Wwy32"
      },
      "id": "vchbuD4Wwy32",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wants to learn the x^2 features so linear is a bad model e.g. direction (+/-) does not matter for falling for pendulum\n",
        "# implement NN to add non-linearity - use softplus for output\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DenseNet, self).__init__()\n",
        "    self.layer1 = nn.Linear(8,20)\n",
        "    self.layer2 = nn.Linear(20,1)\n",
        "    self.softplus = nn.Softplus()\n",
        "  \n",
        "  def forward(self, X):\n",
        "    hidden1 = self.layer1(X)\n",
        "    hidden2 = self.layer2(hidden1)\n",
        "\n",
        "    output = self.softplus(hidden2)\n",
        "    return output"
      ],
      "metadata": {
        "id": "uS2ohSKX8vbv"
      },
      "id": "uS2ohSKX8vbv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense_model = DenseNet()\n",
        "print(dense_model)"
      ],
      "metadata": {
        "id": "7jAj5bWQLxYz"
      },
      "id": "7jAj5bWQLxYz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test fwd pass of NN\n",
        "X_fwd_pass = torch.ones(1, 8, dtype=torch.float32, device='cpu')\n",
        "dense_model(X_fwd_pass)"
      ],
      "metadata": {
        "id": "DS056DSJ2T7q"
      },
      "id": "DS056DSJ2T7q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess data into pytorch tensors\n",
        "\n",
        "dtype = torch.float32\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Using device: \\n', device)\n",
        "\n",
        "train_labels_tensor = torch.tensor(train_labels.values, dtype=dtype, device=device).view(len(train_labels), -1)\n",
        "train_features_tensor = torch.tensor(train_features.values, dtype=dtype, device=device, requires_grad=True)\n",
        "\n",
        "valid_labels_tensor = torch.tensor(valid_labels.values, dtype=dtype, device=device).view(len(valid_labels), -1)\n",
        "valid_features_tensor = torch.tensor(valid_features.values, dtype=dtype, device=device)\n",
        "\n",
        "test_labels_tensor = torch.tensor(test_labels.values, dtype=dtype, device=device).view(len(test_labels), -1)\n",
        "test_features_tensor = torch.tensor(test_features.values, dtype=dtype, device=device)\n",
        "\n",
        "print('Test tensor shapes:', test_labels_tensor.shape, test_features_tensor.shape)\n",
        "print('Validation tensor shapes:', valid_labels_tensor.shape, valid_features_tensor.shape)\n",
        "print('Train tensor shapes:', train_labels_tensor.shape, train_features_tensor.shape)"
      ],
      "metadata": {
        "id": "zU2ORSEtNfn3"
      },
      "id": "zU2ORSEtNfn3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "# Training loop\n",
        "\n",
        "def train(y_train, X_train, y_valid, X_valid, y_test, X_test, model_, \\\n",
        "          model_name, epochs=10000, learning_rate=0.1):\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = torch.optim.SGD(model_.parameters(), lr=learning_rate)\n",
        "  model_.name = model_name\n",
        "  for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    train_preds = model_(X_train)\n",
        "    train_loss = torch.sqrt(criterion(train_preds, y_train))\n",
        "    if epoch % 500 == 0:\n",
        "      valid_preds = model_(X_valid)\n",
        "      valid_loss = torch.sqrt(criterion(valid_preds, y_valid))\n",
        "      print(f'epoch #{epoch} \\n training loss: \\n', train_loss.item(), \\\n",
        "            '\\n validation loss: \\n', valid_loss.item(), '\\n')\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  test_preds = model_(X_test)\n",
        "  test_loss = torch.sqrt(criterion(test_preds, y_test))\n",
        "\n",
        "  print('Final metrics: \\n training loss: ', train_loss.item(), \\\n",
        "        '\\n validation loss: ', valid_loss.item(),'\\n test loss: ', \\\n",
        "        test_loss.item())\n"
      ],
      "metadata": {
        "id": "Q9kS-Yfi0DiZ"
      },
      "id": "Q9kS-Yfi0DiZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model to gpu and train\n",
        "dense_model.to(device)\n",
        "\n",
        "train(train_labels_tensor, train_features_tensor, valid_labels_tensor, valid_features_tensor, \\\n",
        "      test_labels_tensor, test_features_tensor, dense_model, model_name='10000eps_DenseNet_10000epochs', epochs=10000)"
      ],
      "metadata": {
        "id": "UmaC7D0g7EG8"
      },
      "id": "UmaC7D0g7EG8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# next add reLU for additional non-linearity\n",
        "\n",
        "class DenseNetRelu(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DenseNetRelu, self).__init__()\n",
        "    self.layer1 = nn.Linear(8,20)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.layer2 = nn.Linear(20,1)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.softplus = nn.Softplus()\n",
        "  \n",
        "  def forward(self, X):\n",
        "    hidden1 = self.relu1(self.layer1(X))\n",
        "    hidden2 = self.relu2(self.layer2(hidden1))\n",
        "\n",
        "    output = self.softplus(hidden2)\n",
        "    return output"
      ],
      "metadata": {
        "id": "_ILkFZvfWG1y"
      },
      "id": "_ILkFZvfWG1y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense_plus_relu_model = DenseNetRelu()\n",
        "print(dense_plus_relu_model)"
      ],
      "metadata": {
        "id": "gd5p8BwgYNM1"
      },
      "id": "gd5p8BwgYNM1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense_plus_relu_model.to(device)\n",
        "\n",
        "train(train_labels_tensor, train_features_tensor, valid_labels_tensor, valid_features_tensor, \\\n",
        "      test_labels_tensor, test_features_tensor, dense_plus_relu_model, model_name='10000eps_DenseNetRelu_10000epochs', epochs=10000)"
      ],
      "metadata": {
        "id": "eEcpVrPJY1TY"
      },
      "id": "eEcpVrPJY1TY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMnet(nn.Module):\n",
        "  def __init__(self, batch_first=True):\n",
        "    super(LSTMnet, self).__init__()\n",
        "    self.lstm = nn.LSTM(8, 16)\n",
        "    self.linear = nn.Linear(16, 1)\n",
        "    self.softplus = nn.Softplus()\n",
        "  \n",
        "  def forward(self, ep_features):\n",
        "    # embeds.view(len(sentence), 1, -1)) --> ??\n",
        "    batch_size, ep_length, num_feats = ep_features.size()\n",
        "    # used to be ep_features.view(len(ep_features),1,-1)\n",
        "    lstm_out, _ =  self.lstm(ep_features.view(batch_size, -1, num_feats))\n",
        "    # may also need unpad the upper zeroes\n",
        "    # lstm_out, _ = self.lstm(ep_features.view(ep_features.size()[0], ep_features.size()[1],1,-1))\n",
        " # used to be ep_features.view(len(ep_features), -1)\n",
        "    linear_out = self.linear(lstm_out.view(batch_size, -1, 16))\n",
        "    output = self.softplus(linear_out)\n",
        "\n",
        "    return output\n",
        "\n"
      ],
      "metadata": {
        "id": "m-_tRsGDeRyk"
      },
      "id": "m-_tRsGDeRyk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = LSTMnet()\n",
        "lstm_model.to(device)"
      ],
      "metadata": {
        "id": "2x87u37sSKUd"
      },
      "id": "2x87u37sSKUd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare data for lstm i.e. [(ep1_features, ep1_labels), (ep2_features, ep2_labels), ...] in tensors\n",
        "\n",
        "eps = []\n",
        "\n",
        "for ep_num in range(num_episodes):\n",
        "  episode = df[df['ep_num']==ep_num]\n",
        "  episode_feats = torch.tensor(episode[features].values, dtype=dtype, device=device, requires_grad=True)\n",
        "  episode_labs = torch.tensor(episode['time_to_terminate'].values, dtype=dtype, device=device).view(len(episode),-1)\n",
        "  eps.append((episode_feats, episode_labs))\n",
        "\n",
        "assert len(eps) == num_episodes\n",
        "\n",
        "# split into train, validate, test\n",
        "test_episodes = eps[:int(test_size)]\n",
        "valid_episodes = eps[int(test_size): int(test_size)+int(valid_size)]\n",
        "train_episodes = eps[int(test_size)+int(valid_size):]\n",
        "\n",
        "print(len(train_episodes), train_episodes[:10])\n",
        "\n",
        "assert len(test_episodes)+ len(valid_episodes)+len(train_episodes) == len(eps)"
      ],
      "metadata": {
        "id": "R4y9Ko8bjYTw"
      },
      "id": "R4y9Ko8bjYTw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch processing\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class EpisodesDataset(Dataset):\n",
        "    def __init__(self, episode_feats, episode_labs):\n",
        "        self.episode_feats = episode_feats\n",
        "        self.episode_labs = episode_labs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.episode_labs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.episode_feats[idx], self.episode_labs[idx]\n",
        "\n"
      ],
      "metadata": {
        "id": "lEpqTk5-nORD"
      },
      "id": "lEpqTk5-nORD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ep_data_to_pytorch_dataset(ep_data):\n",
        "  all_episode_feats = [ep_data[i][0] for i in range(len(ep_data))]\n",
        "  all_episode_labs = [ep_data[i][1] for i in range(len(ep_data))]\n",
        "  pytorch_dataset = EpisodesDataset(all_episode_feats, all_episode_labs)\n",
        "  return pytorch_dataset\n",
        "\n",
        "train_dataset = ep_data_to_pytorch_dataset(train_episodes)\n",
        "valid_dataset = ep_data_to_pytorch_dataset(valid_episodes)\n",
        "test_dataset = ep_data_to_pytorch_dataset(test_episodes)\n",
        "\n",
        "print(len(train_dataset), len(valid_dataset), len(test_dataset))\n",
        "train_dataset[:2]"
      ],
      "metadata": {
        "id": "v6y0qaRGYAEv"
      },
      "id": "v6y0qaRGYAEv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find max ep length\n",
        "\n",
        "MAX_EP_LENGTH = 1\n",
        "for ep in eps:\n",
        "  if len(ep[0])>MAX_EP_LENGTH:\n",
        "    MAX_EP_LENGTH = len(ep[0])\n",
        "print(MAX_EP_LENGTH)"
      ],
      "metadata": {
        "id": "e9sVK-1WxYaM"
      },
      "id": "e9sVK-1WxYaM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ep_data_to_padded_pytorch_dataset(ep_data, max_ep_length):\n",
        "  padded_all_episode_feats = []\n",
        "  padded_all_episode_labs = []\n",
        "  for i in range(len(ep_data)):\n",
        "    ep_feats = ep_data[i][0]\n",
        "    ep_labs = ep_data[i][1]\n",
        "    if len(ep_feats) != max_ep_length:\n",
        "      num_zeros = max_ep_length - len(ep_feats)\n",
        "      pad_feats_arr = np.zeros((num_zeros, ep_feats.shape[1]))\n",
        "      pad_labs_arr = np.zeros((num_zeros, ep_labs.shape[1]))\n",
        "      ep_feats = torch.cat((torch.tensor(pad_feats_arr, dtype=dtype, device=device), ep_feats), dim=0)\n",
        "      ep_labs = torch.cat((torch.tensor(pad_labs_arr, dtype=dtype, device=device), ep_labs), dim=0)\n",
        "      \n",
        "    padded_all_episode_feats.append(ep_feats)\n",
        "    padded_all_episode_labs.append(ep_labs)\n",
        "  \n",
        "  padded_pytorch_dataset = EpisodesDataset(padded_all_episode_feats, padded_all_episode_labs)\n",
        "  return padded_pytorch_dataset\n",
        "\n",
        "padded_train_dataset = ep_data_to_padded_pytorch_dataset(train_episodes, MAX_EP_LENGTH)\n",
        "padded_valid_dataset = ep_data_to_padded_pytorch_dataset(valid_episodes, MAX_EP_LENGTH)\n",
        "padded_test_dataset = ep_data_to_padded_pytorch_dataset(test_episodes, MAX_EP_LENGTH)\n",
        "\n",
        "print(len(padded_train_dataset), len(padded_valid_dataset), len(padded_test_dataset))\n",
        "padded_train_dataset[:2]"
      ],
      "metadata": {
        "id": "ZfrNSPldrnrk"
      },
      "id": "ZfrNSPldrnrk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pack_sequence\n",
        "\n",
        "def my_collate(batch):\n",
        "    # batch contains a list of tuples of structure (sequence, target)\n",
        "    print(batch)\n",
        "    data = [item[0] for item in batch]\n",
        "    print(data)\n",
        "    data = pack_sequence(data, enforce_sorted=False)\n",
        "    print(data)\n",
        "    targets = [item[1] for item in batch]\n",
        "    print(targets)\n",
        "    return [data, targets]\n"
      ],
      "metadata": {
        "id": "1g_XcFwiqB39"
      },
      "id": "1g_XcFwiqB39",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE=256\n",
        "\n",
        "train_dataloader = DataLoader(padded_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_dataloader = DataLoader(padded_valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataloader = DataLoader(padded_test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")"
      ],
      "metadata": {
        "id": "7T3qP1s6ctdK"
      },
      "id": "7T3qP1s6ctdK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = lstm_model(train_features)\n",
        "print(out.size())"
      ],
      "metadata": {
        "id": "jmfxlqvi4MW7"
      },
      "id": "jmfxlqvi4MW7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try lstm to enable dependency between t, t+1 but only within the same episode\n",
        "# train on gpu\n",
        "from tqdm import tqdm\n",
        "\n",
        "def lstm_train(train_eps, valid_eps, test_eps, model_, model_name, epochs=10000, learning_rate=0.1):\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = torch.optim.SGD(model_.parameters(), lr=learning_rate)\n",
        "  model_.name = model_name\n",
        "  for epoch, _ in enumerate(tqdm(range(epochs))):\n",
        "    start_time = time.time()\n",
        "    train_loss_sum_sq = 0.\n",
        "    for step, (ep_features, ep_labels) in enumerate(train_eps):\n",
        "      optimizer.zero_grad()\n",
        "      ep_preds = model_(ep_features)\n",
        "      ep_loss = torch.sqrt(criterion(ep_preds, ep_labels))\n",
        "\n",
        "      train_loss_sum_sq+=criterion(ep_preds, ep_labels).item()*len(ep_labels)\n",
        "\n",
        "      ep_loss.backward()\n",
        "      optimizer.step()\n",
        "    if epoch % 1000 == 0:\n",
        "      valid_loss_sum_sq = 0.\n",
        "      for step, (ep_features, ep_labels) in enumerate(valid_eps):\n",
        "        valid_preds = model_(ep_features)\n",
        "        valid_loss_sum_sq +=criterion(valid_preds, ep_labels).item()*len(ep_labels)\n",
        "      elapsed_time = time.time() - start_time\n",
        "      print(f'epoch #{epoch} \\n training loss: \\n', np.sqrt(train_loss_sum_sq / len(train_eps)), \\\n",
        "          '\\n validation loss: \\n', np.sqrt(valid_loss_sum_sq / len(valid_eps)), '\\n elapsed time: \\n', elapsed_time)\n",
        "  \n",
        "  test_loss_sum_sq = 0.\n",
        "  for ep_features, ep_labels in test_eps:\n",
        "    test_preds = model_(ep_features)\n",
        "    test_loss_sum_sq +=criterion(test_preds, ep_labels).item()*len(ep_labels)\n",
        "\n",
        "  print('Final metrics: \\n training loss: \\n', np.sqrt(train_loss_sum_sq / len(train_eps)), \\\n",
        "          '\\n validation loss: \\n', np.sqrt(valid_loss_sum_sq / len(valid_eps)), \\\n",
        "        '\\n test loss: \\n', np.sqrt(test_loss_sum_sq / len(test_eps)))\n"
      ],
      "metadata": {
        "id": "-HMXsDNxY-Tu"
      },
      "id": "-HMXsDNxY-Tu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_train(train_dataloader, valid_dataloader, test_dataloader, lstm_model, model_name='10000eps_LSTMnet_10000epochs' epochs=10000)"
      ],
      "metadata": {
        "id": "Sq9giHdonGjn"
      },
      "id": "Sq9giHdonGjn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = test_df.sample()\n",
        "sample_feat = sample[features]\n",
        "sample_lab = sample['time_to_terminate']\n",
        "print('Linear model: ',regr.predict(sample_feat)[0])\n",
        "print('Poisson model: ', pois_regr.predict(sample_feat)[0])\n",
        "print('DenseNet model: ', dense_model(torch.tensor(sample_feat.values, dtype=dtype, device=device)).item())\n",
        "print('DenseNetRelu model: ', dense_plus_relu_model(torch.tensor(sample_feat.values, dtype=dtype, device=device)).item())\n",
        "print('LSTM model: ', lstm_model(torch.tensor(sample_feat.values.reshape(1,len(sample_feat), 8), dtype=dtype, device=device)).item())\n",
        "print('Actual:  ', sample_lab.values[0])"
      ],
      "metadata": {
        "id": "T14-cgqdVr_G"
      },
      "id": "T14-cgqdVr_G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# align loss fn with others\n",
        "# check ep loss fn -> maybe try ep loss for others instead of epoch loss"
      ],
      "metadata": {
        "id": "hbyz97UySOwQ"
      },
      "id": "hbyz97UySOwQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save all models\n",
        "\n",
        "def save_models(model_list, dir):\n",
        "  for model__ in model_list:\n",
        "    model_file_name = f'{model__.name}.pt'\n",
        "    torch.save(model__.state_dict(), dir+'/'+model_file_name)\n",
        "  return ' models saved to directory ', dir"
      ],
      "metadata": {
        "id": "XNH4P8olT4Jw"
      },
      "id": "XNH4P8olT4Jw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models"
      ],
      "metadata": {
        "id": "NhiVk5reulw7"
      },
      "id": "NhiVk5reulw7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_models([dense_model, dense_plus_relu_model, lstm_model], 'models')"
      ],
      "metadata": {
        "id": "WjXiQjDnVdcN"
      },
      "id": "WjXiQjDnVdcN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy models to drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "ROOT = '/content/drive'\n",
        "PROJ = 'MyDrive/LearningPhysics/Models'\n",
        "drive.mount(ROOT)\n"
      ],
      "metadata": {
        "id": "Y-EYZ78VYNaT"
      },
      "id": "Y-EYZ78VYNaT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os.path import join\n",
        "\n",
        "PROJECT_PATH = join(ROOT, PROJ)\n",
        "print(PROJECT_PATH)"
      ],
      "metadata": {
        "id": "DyC_kSSUdqfG"
      },
      "id": "DyC_kSSUdqfG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd models\n",
        "!cp -r /* {PROJECT_PATH}"
      ],
      "metadata": {
        "id": "yBiEC6N_cbw_"
      },
      "id": "yBiEC6N_cbw_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mCYdMc2GfEg9"
      },
      "id": "mCYdMc2GfEg9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}